# -*- coding: utf-8 -*-
"""ClassifiersWith_Oversampl/SMOTE_OneHot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yLPd6ezy_aZYuT0wsRcScNEPibw_V66P

Dans ce notebook on applique le mécanisme de SMOTE et Oversampler pour les algorithme knn, GaussianNB et RandomForest.

Nous appliquons le svm avec l'option de pondération intégré.
Le prétraitement sera avec la transformation oneHotEncoder.
"""

import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import sklearn
import pickle
from pprint import pprint
import sklearn.metrics
from sklearn import *
from sklearn.metrics import *
from sklearn.naive_bayes import GaussianNB
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import *
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.ensemble import RandomForestClassifier

"""# Metrics"""


def Metrics(y_pred , y) :
  #List of Metrics
  list=['micro',None,'weighted','macro']
  evaluate_result = dict()
  for i in list :
    precision, recall, f_score, _ = precision_recall_fscore_support(y, y_pred, average=i)
    evaluate_result[str(i)] = {
            'f_score': f_score,
            'precision': precision,
            'recall': recall,
        }
  return evaluate_result

"""#1 RandomOverSampler"""

# shuffle the dataset and load 
donnees=pd.read_csv("/Donnees/donnees.csv")
non_dem=donnees.loc[donnees['DEM'] == 0]
donnees_=pd.concat([donnees,non_dem])
donnees=donnees_
donnees = donnees.sample(frac=1).reset_index(drop=True)
##print("Data")
##print(donnees)
print(end="\n")
print(end="\n")
dt_features=donnees.copy()
dt_labels=dt_features.copy()
dt_labels=dt_labels.pop('DEM')
###

# one hot encoder
one_hot=OneHotEncoder()
encoder = OneHotEncoder(handle_unknown='ignore')
#perform one-hot encoding on 'team' column 
encoder_df = pd.DataFrame(encoder.fit_transform(dt_features[['CDSITFAM']]).toarray())
final_df = dt_features.join(encoder_df)
encoder_df = pd.DataFrame(encoder.fit_transform(dt_features[['RANGAGEAD']]).toarray())
final_df = dt_features.join(encoder_df)
encoder_df = pd.DataFrame(encoder.fit_transform(dt_features[['RANGADH']]).toarray())
final_df = dt_features.join(encoder_df)
dt_features=final_df.drop(['CDSITFAM','RANGAGEAD','RANGADH','DEM'],axis=1)

print(end="\n")


from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(sampling_strategy="minority") #   not majority
X_res, y_res = ros.fit_resample(dt_features, dt_labels)
y_res.value_counts().plot.pie(autopct='%.2f')
plt.show()
print(dt_features)
X_train, X_rem, y_train, y_rem = train_test_split(X_res,y_res, train_size=0.8) #on sépare les données en train et en test
test_size = 0.5
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5) #test_size=0.5 signifie 50% des données restantes

# implement a naive bayes classifier
print("Naive Bayes")
clf = GaussianNB()
y_train = np.array(y_train)
y_train=y_train.reshape(-1,1)
clf.fit(X_train, y_train)
# predict the response
pred = clf.predict(X_valid)
# evaluate accuracy
print(accuracy_score(y_valid, pred))
# print the confusion matrix
print(confusion_matrix(y_valid, pred))
pprint(Metrics(y_valid,pred))
print(f"Naive Bayes Report : \n\n {classification_report(y_valid, pred)}")
with open('/Modeles/GaussianNB_Oversamp', 'wb') as files:
    pickle.dump(clf, files)



# implement k-nearest neighbors classifier
print("KNN")
knn = KNeighborsClassifier(n_neighbors=3)
# train the classifier
knn.fit(X_train, y_train)
# predict the response
pred = knn.predict(X_valid)
# evaluate accuracy
print(accuracy_score(y_valid, pred))
# print the confusion matrix
print(confusion_matrix(y_valid, pred))
pprint(Metrics(y_valid, pred))
print(f"KNN Report : \n\n {classification_report(y_valid, pred)}")
with open('/Modeles/KNN_Oversamp', 'wb') as files:
    pickle.dump(clf, files)






#Create a Random Forest Classifier
print("RFC")
clf=RandomForestClassifier(n_estimators=10)
clf.fit(X_train,y_train)
pred=clf.predict(X_valid)
# evaluate accuracy
print(accuracy_score(y_valid, pred))
# print the confusion matrix
print(confusion_matrix(y_valid, pred))
pprint(Metrics(y_valid, pred))
print(f"Random Forest Report : \n\n {classification_report(y_valid, pred)}")
with open('/Modeles/RandomForest_Oversamp', 'wb') as files:
    pickle.dump(clf, files)

#test after oversampling
# load saved model
with open('/Modeles/RandomForest_Oversamp' , 'rb') as f:
    RandomForest_Oversamp = pickle.load(f)

pred = clf.predict(X_test)
print(accuracy_score(y_test, pred))

"""#2 SMOTE
Utilisation du smote 

Le principe de **SMOTE** est simple : produire de nouveaux individus minoritaires qui ressemblent aux autres, sans être strictement identiques. Cela permet de densifier la population d'individus minoritaires de manière plus homogène.

C'est sur ces données transformées, auxquelles nous avons ajouté de faux individus synthétiques, que nous allons ensuite entraîner un modèle de Machine Learning.

Dans l’algorithme du SMOTE, deux paramètres essentiels interviennent :

k, le nombre de plus proches voisins (nearest neighbors) candidats pour la création d’un nouvel individu
"""

# Importation du package
from imblearn.over_sampling import SMOTE
donnees=pd.read_csv("/Donnees/donnees.csv")
donnees = donnees.sample(frac=1).reset_index(drop=True)
x=donnees.copy()
y=x.copy()
y=y.pop('DEM')

# one hot encoder
one_hot=OneHotEncoder()
encoder = OneHotEncoder(handle_unknown='ignore')

#perform one-hot encoding on 'team' column 
encoder_df = pd.DataFrame(encoder.fit_transform(x[['CDSITFAM']]).toarray())
final_df = x.join(encoder_df)
encoder_df = pd.DataFrame(encoder.fit_transform(x[['RANGAGEAD']]).toarray())
final_df = x.join(encoder_df)
encoder_df = pd.DataFrame(encoder.fit_transform(x[['RANGADH']]).toarray())
final_df = x.join(encoder_df)

x=final_df.drop(['CDSITFAM','RANGAGEAD','RANGADH','DEM'],axis=1)


# Définition de l'instance SMOTE
sm = SMOTE(sampling_strategy='minority')
# Application du SMOTE aux données
X_sm, y_sm = sm.fit_resample(x, y)
y_sm.value_counts().plot.pie(autopct='%.2f')
plt.show()


X_train, X_rem, y_train, y_rem = train_test_split(X_sm,y_sm, train_size=0.8) #on sépare les données en train et en test
test_size = 0.5
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5) #test_size=0.5 signifie 50% des données restantes




# implement a naive bayes classifier
print("Naive Bayes")
clf = GaussianNB()
# clf = ComplementNB()
# train the classifier
y_train = np.array(y_train)
y_train=y_train.reshape(-1,1)
clf.fit(X_train, y_train)
# predict the response
pred = clf.predict(X_valid)
# evaluate accuracy
print(accuracy_score(y_valid, pred))
# print the confusion matrix
print(confusion_matrix(y_valid, pred))
pprint(Metrics(y_valid, pred))
print(f"Naive Bayes Report : \n\n {classification_report(y_valid, pred)}")
with open('/Modeles/GaussianNB_SMOTE', 'wb') as files:
    pickle.dump(clf, files)


# implement k-nearest neighbors classifier
print("KNN")
knn = KNeighborsClassifier(n_neighbors=3)
# train the classifier
knn.fit(X_train, y_train)
# predict the response
pred = knn.predict(X_valid)
# evaluate accuracy
print(accuracy_score(y_valid, pred))
# print the confusion matrix
print(confusion_matrix(y_valid, pred))
pprint(Metrics(y_valid, pred))
pred_train= knn.predict(X_train)
print(f"KNN Report : \n\n {classification_report(y_valid, pred)}")
with open('/Modeles/KNN_SMOTE', 'wb') as files:
    pickle.dump(clf, files)



#Create a Random Forest Classifier
print("RFC")
clf=RandomForestClassifier(n_estimators=10)
clf.fit(X_train,y_train)
pred=clf.predict(X_valid)
# evaluate accuracy
print(accuracy_score(y_valid, pred))
# print the confusion matrix
print(confusion_matrix(y_valid, pred))
pprint(Metrics(y_valid, pred))
print(f"Random Forest Report : \n\n {classification_report(y_valid, pred)}")
with open('/Modeles/RandomForest_SMOTE', 'wb') as files:
    pickle.dump(clf, files)

#test after SMOTE
# load saved model
with open('/Modeles/RandomForest_SMOTE' , 'rb') as f:
    RandomForest_SMOTE = pickle.load(f)

pred = RandomForest_SMOTE.predict(X_test)
print(accuracy_score(y_test, pred))

"""#WeightedSVM"""

donnees=pd.read_csv("/Donnees/donnees.csv")
non_dem=donnees.loc[donnees['DEM'] == 0]
donnees_=pd.concat([donnees,non_dem])
donnees=donnees_
# shuffle the dataset
donnees = donnees.sample(frac=1).reset_index(drop=True)
print("Data")
print(donnees)
print(end="\n")
print(end="\n")
########
dt_features_1=donnees.copy()
dt_labels=dt_features_1.copy()
dt_labels=dt_labels.pop('DEM')
########
########
# one hot encoder
one_hot=OneHotEncoder()
encoder = OneHotEncoder(handle_unknown='ignore')
#perform one-hot encoding on 'team' column 
encoder_df = pd.DataFrame(encoder.fit_transform(dt_features_1[['CDSITFAM']]).toarray())
final_df = dt_features_1.join(encoder_df)
encoder_df = pd.DataFrame(encoder.fit_transform(dt_features_1[['RANGAGEAD']]).toarray())
final_df = dt_features_1.join(encoder_df)
encoder_df = pd.DataFrame(encoder.fit_transform(dt_features_1[['RANGADH']]).toarray())
final_df = dt_features_1.join(encoder_df)
dt_features_1=final_df.drop(['CDSITFAM','RANGAGEAD','RANGADH','DEM'],axis=1)

print(dt_features_1)
X_train, X_rem, y_train, y_rem = train_test_split(dt_features_1,dt_labels, train_size=0.8) #on sépare les données en train et en test
test_size = 0.5
X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5) #test_size=0.5 signifie 50% des données restantes




# create a SVM classifier
print("SVM")
name_clf="svm"
#demissioner => 1 pas dem => 0 nous allons donner un poid plus grand a la classe minoritaire 
weights = {1:0.1, 0:1.0}
SVM_ = svm.SVC(gamma='auto', class_weight=weights)
#clf = svm.SVC(gamma='auto')
y_train = np.array(y_train)
y_train.reshape(-1,1)
SVM_.fit(X_train, y_train)
pred=SVM_.predict(X_valid)
y_tr=SVM_.predict(X_train)
# evaluate accuracy
print(accuracy_score(y_valid, pred))
# print the confusion matrix
print(confusion_matrix(y_valid, pred))

pprint(Metrics(y_valid, pred))
pprint(Metrics(y_train, y_tr))
#print(f"Classifier Report : \n\n {classification_report(y_valid, pred)}")
with open('/Modeles/SVM_ohePreprocess', 'wb') as files:
    pickle.dump(SVM_, files)